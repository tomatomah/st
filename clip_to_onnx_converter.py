import json
import os
import time

import clip
import numpy as np
import onnx
import onnxruntime as ort
import torch
import torchvision.transforms as transforms
from PIL import Image


class BaseStaffDetector:
    """ã‚¹ã‚¿ãƒƒãƒ•åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, threshold=0.6):
        self.threshold = threshold
        self.uniform_features = None
        self.uniform_name = None

    def _load_image(self, image_path):
        """ç”»åƒèª­ã¿è¾¼ã¿"""
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
        return Image.open(image_path).convert("RGB")

    def _calculate_similarity(self, features1, features2):
        """é¡ä¼¼åº¦è¨ˆç®—ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError

    def register_uniform(self, image_path, name="staff_uniform"):
        """åˆ¶æœç”»åƒç™»éŒ²"""
        print(f"åˆ¶æœç™»éŒ²ä¸­: {image_path}")
        self.uniform_features = self._extract_features(image_path)
        self.uniform_name = name
        print(f"åˆ¶æœç™»éŒ²å®Œäº†: {name}")

    def detect_staff(self, image_path):
        """ã‚¹ã‚¿ãƒƒãƒ•åˆ¤å®š"""
        if self.uniform_features is None:
            raise ValueError("åˆ¶æœç”»åƒãŒæœªç™»éŒ²ã§ã™")

        person_features = self._extract_features(image_path)
        similarity = self._calculate_similarity(person_features, self.uniform_features)

        return {
            "is_staff": similarity > self.threshold,
            "similarity": similarity,
            "uniform_name": self.uniform_name,
            "threshold": self.threshold,
        }


class SimpleStaffDetector(BaseStaffDetector):
    """PyTorchç‰ˆã‚¹ã‚¿ãƒƒãƒ•åˆ¤å®š"""

    def __init__(self, model_name="RN50", device="cpu", threshold=0.6):
        super().__init__(threshold)
        print(f"Loading CLIP model: {model_name}")
        self.model, self.preprocess = clip.load(model_name, device=device)
        self.device = device
        torch.set_num_threads(4)
        torch.set_grad_enabled(False)

    def _extract_features(self, image_path):
        """ç‰¹å¾´é‡æŠ½å‡º"""
        image = self._load_image(image_path)
        image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)

        with torch.no_grad():
            features = self.model.encode_image(image_tensor)
            return features / features.norm(dim=-1, keepdim=True)

    def _calculate_similarity(self, features1, features2):
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        return torch.cosine_similarity(features1, features2).item()


class OptimizedStaffDetector(BaseStaffDetector):
    """ONNXç‰ˆé«˜é€Ÿã‚¹ã‚¿ãƒƒãƒ•åˆ¤å®š"""

    def __init__(self, onnx_path="clip_image_encoder.onnx", threshold=0.6):
        super().__init__(threshold)
        print("Loading ONNX model...")
        self.session = ort.InferenceSession(onnx_path, providers=["CPUExecutionProvider"])
        self.preprocess = self._create_preprocess()
        print("ONNX model loaded")

    def _create_preprocess(self):
        """å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆCLIPè¨­å®š
        config = {
            "resize": 224,
            "center_crop": 224,
            "mean": [0.48145466, 0.4578275, 0.40821073],
            "std": [0.26862954, 0.26130258, 0.27577711],
        }

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
        if os.path.exists("clip_preprocess_config.json"):
            with open("clip_preprocess_config.json", "r") as f:
                config.update(json.load(f))

        return transforms.Compose(
            [
                transforms.Resize(config["resize"], interpolation=transforms.InterpolationMode.BICUBIC),
                transforms.CenterCrop(config["center_crop"]),
                transforms.ToTensor(),
                transforms.Normalize(mean=config["mean"], std=config["std"]),
            ]
        )

    def _extract_features(self, image_path):
        """ONNXç‰¹å¾´é‡æŠ½å‡º"""
        image = self._load_image(image_path)
        image_array = self.preprocess(image).unsqueeze(0).numpy()

        # ONNXæ¨è«–
        input_name = self.session.get_inputs()[0].name
        features = self.session.run(None, {input_name: image_array})[0]

        # L2æ­£è¦åŒ–
        norm = np.linalg.norm(features, axis=1, keepdims=True)
        return features / (norm + 1e-8)

    def _calculate_similarity(self, features1, features2):
        """NumPyé¡ä¼¼åº¦è¨ˆç®—"""
        return np.dot(features1, features2.T).item()


class CLIPConverter:
    """CLIPâ†’ONNXå¤‰æ›å™¨"""

    @staticmethod
    def convert(model_name="RN50", output_path="clip_image_encoder.onnx"):
        """ONNXå¤‰æ›å®Ÿè¡Œ"""
        print(f"Converting {model_name} to ONNX...")

        try:
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model, _ = clip.load(model_name, device="cpu", jit=False)
            model.visual.eval()

            # ONNXå¤‰æ›
            dummy_input = torch.randn(1, 3, 224, 224)
            torch.onnx.export(
                model.visual,
                dummy_input,
                output_path,
                export_params=True,
                opset_version=17,
                input_names=["image"],
                output_names=["features"],
                dynamic_axes={"image": {0: "batch_size"}, "features": {0: "batch_size"}},
            )

            # æ¤œè¨¼
            onnx.checker.check_model(onnx.load(output_path))

            # è¨­å®šä¿å­˜
            CLIPConverter._save_config()

            print(f"âœ… å¤‰æ›å®Œäº†: {output_path}")
            return True

        except Exception as e:
            print(f"âŒ å¤‰æ›å¤±æ•—: {e}")
            return False

    @staticmethod
    def _save_config():
        """å‰å‡¦ç†è¨­å®šä¿å­˜"""
        config = {
            "resize": 224,
            "center_crop": 224,
            "mean": [0.48145466, 0.4578275, 0.40821073],
            "std": [0.26862954, 0.26130258, 0.27577711],
        }
        with open("clip_preprocess_config.json", "w") as f:
            json.dump(config, f, indent=2)


def benchmark(uniform_path="uniform.jpg", iterations=10):
    """æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    if not os.path.exists(uniform_path):
        print(f"âŒ ãƒ†ã‚¹ãƒˆç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {uniform_path}")
        return

    print("=== æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆ ===")

    # PyTorchç‰ˆãƒ†ã‚¹ãƒˆ
    print("\n1. PyTorchç‰ˆ")
    pytorch_detector = SimpleStaffDetector()
    pytorch_detector.register_uniform(uniform_path)

    pytorch_times = []
    for _ in range(iterations):
        start = time.time()
        pytorch_result = pytorch_detector.detect_staff(uniform_path)
        pytorch_times.append(time.time() - start)

    # ONNXç‰ˆãƒ†ã‚¹ãƒˆ
    print("\n2. ONNXç‰ˆ")
    if not os.path.exists("clip_image_encoder.onnx"):
        print("âŒ ONNXãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    onnx_detector = OptimizedStaffDetector()
    onnx_detector.register_uniform(uniform_path)

    onnx_times = []
    for _ in range(iterations):
        start = time.time()
        onnx_result = onnx_detector.detect_staff(uniform_path)
        onnx_times.append(time.time() - start)

    # çµæœè¡¨ç¤º
    pytorch_avg = np.mean(pytorch_times) * 1000
    onnx_avg = np.mean(onnx_times) * 1000
    speedup = pytorch_avg / onnx_avg

    print(f"\nğŸ“Š çµæœ:")
    print(f"PyTorch: {pytorch_avg:.2f}ms")
    print(f"ONNX:    {onnx_avg:.2f}ms")
    print(f"é«˜é€ŸåŒ–:  {speedup:.2f}x")
    print(f"ç²¾åº¦å·®:  {abs(pytorch_result['similarity'] - onnx_result['similarity']):.6f}")


def demo():
    """ç°¡å˜ãªãƒ‡ãƒ¢"""
    print("=== CLIP ã‚¹ã‚¿ãƒƒãƒ•åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ  ===")

    # ONNXå¤‰æ›
    if not os.path.exists("clip_image_encoder.onnx"):
        print("ONNXãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›ä¸­...")
        if not CLIPConverter.convert():
            return

    # ãƒ‡ãƒ¢å®Ÿè¡Œ
    detector = OptimizedStaffDetector()

    uniform_path = input("åˆ¶æœç”»åƒãƒ‘ã‚¹: ").strip()
    if uniform_path and os.path.exists(uniform_path):
        detector.register_uniform(uniform_path)

        person_path = input("åˆ¤å®šç”»åƒãƒ‘ã‚¹: ").strip()
        if person_path and os.path.exists(person_path):
            result = detector.detect_staff(person_path)
            status = "ã‚¹ã‚¿ãƒƒãƒ•" if result["is_staff"] else "éã‚¹ã‚¿ãƒƒãƒ•"
            print(f"\nçµæœ: {status} (é¡ä¼¼åº¦: {result['similarity']:.4f})")
        else:
            print("âŒ åˆ¤å®šç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    else:
        print("âŒ åˆ¶æœç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    commands = {
        "1": ("ONNXå¤‰æ›", lambda: CLIPConverter.convert()),
        "2": ("ãƒ‡ãƒ¢å®Ÿè¡Œ", demo),
        "3": ("æ€§èƒ½æ¯”è¼ƒ", benchmark),
        "4": ("çµ‚äº†", lambda: None),
    }

    while True:
        print("\n=== ãƒ¡ãƒ‹ãƒ¥ãƒ¼ ===")
        for key, (desc, _) in commands.items():
            print(f"{key}. {desc}")

        choice = input("\né¸æŠ: ").strip()

        if choice in commands:
            if choice == "4":
                break
            commands[choice][1]()
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠ")


if __name__ == "__main__":
    main()
